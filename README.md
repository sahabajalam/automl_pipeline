# Autonomous ML Pipeline

A comprehensive, automated machine learning pipeline that handles the entire ML workflow from data ingestion to model deployment.

## Features

- ğŸ”„ **Autonomous Workflow**: Fully automated ML pipeline using LangGraph
- ğŸ“Š **Data Processing**: Intelligent data cleaning, validation, and feature engineering
- ğŸ¤– **Multi-Model Training**: Trains and compares multiple ML models automatically
- ğŸš€ **Auto Deployment**: Generates production-ready FastAPI applications with Docker
- ğŸ“ˆ **Monitoring**: Built-in model monitoring and observability
- ğŸ” **Quality Assurance**: Comprehensive data validation and model evaluation

## Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd automl_pipeline

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Pipeline

#### Command Line Interface

```bash
# Basic usage
python main.py --data-path data/sample/titanic.csv --target-column Survived

# With custom project name
python main.py --data-path data/sample/titanic.csv --target-column Survived --project-name my_ml_project

# With custom configuration
python main.py --data-path data/sample/titanic.csv --target-column Survived --config config.json
```

#### Python API

```python
import asyncio
from src.pipeline import AutonomousMLPipeline

async def run_ml_pipeline():
    pipeline = AutonomousMLPipeline()
    
    result = await pipeline.run_pipeline(
        data_path="data/sample/titanic.csv",
        target_column="Survived",
        project_name="titanic_survival_prediction"
    )
    
    print("Pipeline Result:", result)

# Run the pipeline
asyncio.run(run_ml_pipeline())
```

#### REST API

```bash
# Start the API server
python src/api/main.py

# Run pipeline via API
curl -X POST "http://localhost:8000/pipeline/run" \
     -H "Content-Type: application/json" \
     -d '{"data_path": "data/sample/titanic.csv", "target_column": "Survived"}'

# Check status
curl "http://localhost:8000/pipeline/status/your_project_name"
```

### 3. Test the System

```bash
# Run the test script
python test_pipeline.py
```

## Project Structure

```
automl_pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # Pipeline agents
â”‚   â”‚   â”œâ”€â”€ data_agent.py      # Data ingestion & validation
â”‚   â”‚   â”œâ”€â”€ feature_agent.py   # Feature engineering
â”‚   â”‚   â”œâ”€â”€ model_agent.py     # Model training & evaluation
â”‚   â”‚   â””â”€â”€ deployment_agent.py # Model deployment
â”‚   â”œâ”€â”€ api/              # REST API
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ utils/            # Utilities
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â””â”€â”€ pipeline.py       # Main pipeline orchestrator
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw data files
â”‚   â”œâ”€â”€ processed/        # Processed data
â”‚   â””â”€â”€ sample/           # Sample datasets
â”œâ”€â”€ models/               # Trained models
â”œâ”€â”€ deployments/          # Deployment artifacts
â”œâ”€â”€ logs/                 # Pipeline logs
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”œâ”€â”€ tests/                # Test files
â”œâ”€â”€ docker/               # Docker configurations
â”œâ”€â”€ main.py               # CLI entry point
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ config.json          # Default configuration
```

## Pipeline Stages

### 1. Data Ingestion & Validation
- Loads data from various formats (CSV, Excel, JSON, Parquet)
- Validates data quality and structure
- Performs basic data profiling

### 2. Data Cleaning
- Handles missing values intelligently
- Removes duplicates and outliers
- Fixes data type inconsistencies

### 3. Feature Engineering
- Encodes categorical variables
- Creates datetime features
- Generates interaction and polynomial features
- Performs feature selection
- Scales numeric features

### 4. Model Training
- Trains multiple ML models:
  - Random Forest
  - Gradient Boosting (XGBoost, LightGBM)
  - Logistic/Linear Regression
  - Support Vector Machines
  - Neural Networks
- Performs hyperparameter tuning
- Uses cross-validation for robust evaluation

### 5. Model Evaluation
- Comprehensive performance metrics
- Model interpretation and explainability
- Cross-model comparison
- Performance requirement validation

### 6. Deployment
- Generates FastAPI application
- Creates Docker configuration
- Sets up monitoring and logging
- Provides deployment scripts

## Configuration

The pipeline can be configured through:

1. **Configuration File** (`config.json`)
2. **Environment Variables**
3. **Command Line Arguments**

### Key Configuration Options

```json
{
  "training": {
    "TEST_SIZE": 0.2,
    "CV_FOLDS": 5,
    "MAX_TRAINING_TIME": 3600
  },
  "thresholds": {
    "MIN_ACCURACY": 0.7,
    "MIN_F1_SCORE": 0.65
  },
  "feature_engineering": {
    "MAX_FEATURES": 50,
    "CORRELATION_THRESHOLD": 0.9
  }
}
```

## Model Deployment

The pipeline automatically generates:

- **FastAPI Application**: Production-ready API with automatic documentation
- **Docker Configuration**: Complete containerization setup
- **Monitoring Setup**: Prometheus metrics and Grafana dashboards
- **API Documentation**: Interactive Swagger/ReDoc documentation

### Deployment Structure

```
deployments/your_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py           # FastAPI application
â”‚   â”œâ”€â”€ model.py          # Model inference module
â”‚   â”œâ”€â”€ schemas.py        # Pydantic models
â”‚   â””â”€â”€ config.py         # Application configuration
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ model.pkl         # Trained model
â”‚   â”œâ”€â”€ model_metadata.json
â”‚   â””â”€â”€ label_encoder.pkl (if needed)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ API.md
    â””â”€â”€ DEPLOYMENT.md
```

## Monitoring

Built-in monitoring includes:

- **API Metrics**: Request count, latency, error rates
- **Model Metrics**: Prediction confidence, drift detection
- **System Metrics**: CPU, memory, disk usage
- **Custom Dashboards**: Grafana visualizations
- **Alerting**: Automated alerts for issues

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

This project is licensed under the MIT License.

## Support

For issues and questions:
1. Check the documentation
2. Review the logs in `logs/`
3. Run the test script: `python test_pipeline.py`
4. Open an issue on GitHub

## Examples

### Classification Example (Titanic)

```bash
python main.py --data-path data/sample/titanic.csv --target-column Survived
```

### Regression Example

```bash
python main.py --data-path data/sample/housing.csv --target-column price
```

### Custom Configuration

```bash
python main.py \
  --data-path data/my_data.csv \
  --target-column target \
  --config my_config.json \
  --project-name my_custom_project
```